{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import pymssql\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.externals import joblib\n",
    "from tkinter import *\n",
    "from tkinter import ttk\n",
    "from tkinter import messagebox\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from time import sleep\n",
    "import _mssql\n",
    "\n",
    "\n",
    "# 서버 로그인 정보 json 파일 불러오기\n",
    "login_data=open('C:/Users/dap/login.json').read()\n",
    "login = json.loads(login_data)\n",
    "\n",
    "use_datetime_data=open('C:/Users/dap/use_datetime.json').read()\n",
    "use_datetime = json.loads(use_datetime_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 서버 연결\n",
    "    cnxn = pymssql.connect(server = login['server'], user = login['user'], password = login['password'],database = login['database'])  \n",
    "    cursor = cnxn.cursor()\n",
    "\n",
    "    # DB WeatherFactor 불러오기\n",
    "    cmd_WF = \"select * from WeatherFactor order by TimeStamp\"\n",
    "    cursor.execute(cmd_WF)\n",
    "\n",
    "    A = pd.read_sql(cmd_WF, cnxn)\n",
    "\n",
    "    # DB YardFactor 불러오기\n",
    "    cmd_YF = \"select * from YardFactor order by TimeStamp\"\n",
    "    cursor.execute(cmd_YF)\n",
    "\n",
    "    B = pd.read_sql(cmd_YF, cnxn)\n",
    "\n",
    "    # DB airkorea 및 reference data 불러오기\n",
    "    cmd_REF = \"select * from ReferenceData order by TimeStamp\"\n",
    "    cursor.execute(cmd_REF)\n",
    "\n",
    "    C = pd.read_sql(cmd_REF, cnxn)\n",
    "\n",
    "    WF = A[A['TimeStamp'] > use_datetime['start']]\n",
    "    YF = B[B['TimeStamp'] > use_datetime['start']]\n",
    "    REF = C[C['TimeStamp'] > use_datetime['start']]\n",
    "\n",
    "    REF = REF[['TimeStamp', 'PM10Yard', 'PM10AirKorea']].reset_index(drop=True)\n",
    "    WF = WF.reset_index(drop=True)\n",
    "    YF = YF.reset_index(drop=True)\n",
    "\n",
    "    # microseconds 삭제\n",
    "    WF['TimeStamp'] = WF['TimeStamp'].apply(lambda x: x.replace(microsecond=0))\n",
    "    YF['TimeStamp'] = YF['TimeStamp'].apply(lambda x: x.replace(microsecond=0))\n",
    "    REF['TimeStamp'] = REF['TimeStamp'].apply(lambda x: x.replace(microsecond=0))\n",
    "\n",
    "    data = pd.merge(REF,WF)\n",
    "    data = pd.merge(data,YF)\n",
    "\n",
    "    data = data[~(data['Temperature'] > 40)]\n",
    "    data = data[~(data['Temperature'] < 0)]\n",
    "    data = data[~(data['WindSpeed'] > 10)]\n",
    "    data = data[~(data['VisibilityDistance'] < 100)]\n",
    "    data = data[~(data['PM10AirKorea'] ==0)]\n",
    "    data = data[~(data['PM10Yard'] == 0)]\n",
    "\n",
    "    datetime = pd.DatetimeIndex(data['TimeStamp'])\n",
    "    data['month'] = datetime.month\n",
    "    data['hour'] = datetime.hour\n",
    "    data['dayofweek'] = datetime.dayofweek\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names= ['PM10AirKorea', 'Temperature',\n",
    "               'RelativeHumidity', 'WindSpeed', 'WindDirection', 'SolarIrradiation',\n",
    "               'AtmospherePressure', 'DustCurrent', 'VisibilityDistance',\n",
    "               'YardVibration', 'SurfaceTemperature', 'month', 'hour', 'dayofweek']\n",
    "\n",
    "X = data[feature_names]\n",
    "Y = data['PM10Yard']\n",
    "\n",
    "X_train = X\n",
    "Y_train = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM \n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense \n",
    "import keras.backend as K \n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 34)                4896      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 35        \n",
      "=================================================================\n",
      "Total params: 4,931\n",
      "Trainable params: 4,931\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "modela = Sequential()\n",
    "modela.add(LSTM(34, input_shape=(14,1)))\n",
    "modela.add(Dense(1))\n",
    "modela.compile(loss='mean_squared_error', optimizer='adam')\n",
    "modela.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_x = X_train.values\n",
    "Y_train_x = Y_train.values\n",
    "\n",
    "X_train_x = X_train_x.reshape(X_train_x.shape[0],14,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected lstm_1_input to have 3 dimensions, but got array with shape (4761, 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-0e0fd94950df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mearly_stop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodela\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    948\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    951\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    747\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 749\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    751\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    125\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    128\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected lstm_1_input to have 3 dimensions, but got array with shape (4761, 14)"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='loss', patience=1, verbose=1)\n",
    "\n",
    "modela.fit(X_train, Y_train, batch_size=30, verbose=1, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a based model a\n",
    "modela = RandomForestRegressor(random_state=105,\n",
    "                              n_estimators=200,\n",
    "                              max_features=5,\n",
    "                              max_depth=30)\n",
    "\n",
    "# param_grid = { \n",
    "#     'n_estimators': [100,200],\n",
    "#     'max_features': ['auto', 'sqrt'],\n",
    "#      'max_features': [5,7,10,14],\n",
    "#     'max_depth' : [10,20,30,40,50]\n",
    "# }\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# cv_modela = GridSearchCV(estimator=modela, param_grid=param_grid, cv= 5)\n",
    "# cv_modela.fit(X_train, Y_train)\n",
    "# cv_modela.best_params_\n",
    "\n",
    "modela.fit(X_train, Y_train)\n",
    "\n",
    "filename = 'C:/Users/dap/model_1.sav'\n",
    "joblib.dump(modela, filename)\n",
    "\n",
    "model_a_score = modela.score(X_train, Y_train)\n",
    "model_a_score = model_a_score *100\n",
    "\n",
    "modelb = RandomForestRegressor(random_state=105,\n",
    "                              n_estimators=200,\n",
    "                              max_features=10,\n",
    "                              max_depth=30)\n",
    "\n",
    "modelb.fit(X_train, Y_train)\n",
    "\n",
    "filename = 'C:/Users/dap/model_2.sav'\n",
    "joblib.dump(modelb, filename)\n",
    "\n",
    "model_b_score = modelb.score(X_train, Y_train)\n",
    "model_b_score = model_b_score *100\n",
    "\n",
    "modelc = DecisionTreeRegressor(random_state=105,\n",
    "                              min_samples_leaf=1,\n",
    "                              max_depth=5)\n",
    "\n",
    "# param_grid = { 'min_samples_leaf':[1,2,3],\n",
    "#            'min_samples_split':[0.01,0.05,0.5,2],\n",
    "#     'max_depth' : [10,20,30,40,50]\n",
    "# }\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# cv_modelc = GridSearchCV(estimator=modelc, param_grid=param_grid, cv= 5)\n",
    "# cv_modelc.fit(X_train, Y_train)\n",
    "# cv_modelc.best_params_\n",
    "\n",
    "modelc.fit(X_train, Y_train)\n",
    "\n",
    "filename = 'C:/Users/dap/model_3.sav'\n",
    "joblib.dump(modelc, filename)\n",
    "\n",
    "model_c_score = modelc.score(X_train, Y_train)\n",
    "model_c_score = model_c_score *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modeling():\n",
    "    \n",
    "    # 서버 연결\n",
    "    cnxn = pymssql.connect(server = login['server'], user = login['user'], password = login['password'],database = login['database'])  \n",
    "    cursor = cnxn.cursor()\n",
    "\n",
    "    # DB WeatherFactor 불러오기\n",
    "    cmd_WF = \"select * from WeatherFactor order by TimeStamp\"\n",
    "    cursor.execute(cmd_WF)\n",
    "\n",
    "    A = pd.read_sql(cmd_WF, cnxn)\n",
    "\n",
    "    # DB YardFactor 불러오기\n",
    "    cmd_YF = \"select * from YardFactor order by TimeStamp\"\n",
    "    cursor.execute(cmd_YF)\n",
    "\n",
    "    B = pd.read_sql(cmd_YF, cnxn)\n",
    "\n",
    "    # DB airkorea 및 reference data 불러오기\n",
    "    cmd_REF = \"select * from ReferenceData order by TimeStamp\"\n",
    "    cursor.execute(cmd_REF)\n",
    "\n",
    "    C = pd.read_sql(cmd_REF, cnxn)\n",
    "\n",
    "    WF = A[A['TimeStamp'] > use_datetime['start']]\n",
    "    YF = B[B['TimeStamp'] > use_datetime['start']]\n",
    "    REF = C[C['TimeStamp'] > use_datetime['start']]\n",
    "\n",
    "    REF = REF[['TimeStamp', 'PM10Yard', 'PM10AirKorea']].reset_index(drop=True)\n",
    "    WF = WF.reset_index(drop=True)\n",
    "    YF = YF.reset_index(drop=True)\n",
    "\n",
    "    # microseconds 삭제\n",
    "    WF['TimeStamp'] = WF['TimeStamp'].apply(lambda x: x.replace(microsecond=0))\n",
    "    YF['TimeStamp'] = YF['TimeStamp'].apply(lambda x: x.replace(microsecond=0))\n",
    "    REF['TimeStamp'] = REF['TimeStamp'].apply(lambda x: x.replace(microsecond=0))\n",
    "\n",
    "    data = pd.merge(REF,WF)\n",
    "    data = pd.merge(data,YF)\n",
    "\n",
    "    data = data[~(data['Temperature'] > 40)]\n",
    "    data = data[~(data['Temperature'] < 0)]\n",
    "    data = data[~(data['WindSpeed'] > 10)]\n",
    "    data = data[~(data['VisibilityDistance'] < 100)]\n",
    "    data = data[~(data['PM10AirKorea'] ==0)]\n",
    "    data = data[~(data['PM10Yard'] == 0)]\n",
    "\n",
    "    datetime = pd.DatetimeIndex(data['TimeStamp'])\n",
    "    data['month'] = datetime.month\n",
    "    data['hour'] = datetime.hour\n",
    "    data['dayofweek'] = datetime.dayofweek\n",
    "\n",
    "    feature_names= ['PM10AirKorea', 'Temperature',\n",
    "               'RelativeHumidity', 'WindSpeed', 'WindDirection', 'SolarIrradiation',\n",
    "               'AtmospherePressure', 'DustCurrent', 'VisibilityDistance',\n",
    "               'YardVibration', 'SurfaceTemperature', 'month', 'hour', 'dayofweek']\n",
    "\n",
    "    X = data[feature_names]\n",
    "    Y = data['PM10Yard']\n",
    "\n",
    "    X_train = X\n",
    "    Y_train = Y\n",
    "\n",
    "   # Create a based model a\n",
    "    modela = RandomForestRegressor(random_state=105,\n",
    "                                  n_estimators=200,\n",
    "                                  max_features=5,\n",
    "                                  max_depth=30)\n",
    "\n",
    "    # param_grid = { \n",
    "    #     'n_estimators': [100,200],\n",
    "    #     'max_features': ['auto', 'sqrt'],\n",
    "    #      'max_features': [5,7,10,14],\n",
    "    #     'max_depth' : [10,20,30,40,50]\n",
    "    # }\n",
    "    # from sklearn.model_selection import GridSearchCV\n",
    "    # cv_modela = GridSearchCV(estimator=modela, param_grid=param_grid, cv= 5)\n",
    "    # cv_modela.fit(X_train, Y_train)\n",
    "    # cv_modela.best_params_\n",
    "\n",
    "    modela.fit(X_train, Y_train)\n",
    "\n",
    "    filename = 'C:/Users/dap/model_1.sav'\n",
    "    joblib.dump(modela, filename)\n",
    "\n",
    "    model_a_score = modela.score(X_train, Y_train)\n",
    "    model_a_score = model_a_score *100\n",
    "\n",
    "    modelb = RandomForestRegressor(random_state=105,\n",
    "                                  n_estimators=200,\n",
    "                                  max_features=10,\n",
    "                                  max_depth=30)\n",
    "\n",
    "    modelb.fit(X_train, Y_train)\n",
    "\n",
    "    filename = 'C:/Users/dap/model_2.sav'\n",
    "    joblib.dump(modelb, filename)\n",
    "\n",
    "    model_b_score = modelb.score(X_train, Y_train)\n",
    "    model_b_score = model_b_score *100\n",
    "\n",
    "    modelc = DecisionTreeRegressor(random_state=105,\n",
    "                                  min_samples_leaf=1,\n",
    "                                  max_depth=5)\n",
    "\n",
    "    # param_grid = { 'min_samples_leaf':[1,2,3],\n",
    "    #            'min_samples_split':[0.01,0.05,0.5,2],\n",
    "    #     'max_depth' : [10,20,30,40,50]\n",
    "    # }\n",
    "    # from sklearn.model_selection import GridSearchCV\n",
    "    # cv_modelc = GridSearchCV(estimator=modelc, param_grid=param_grid, cv= 5)\n",
    "    # cv_modelc.fit(X_train, Y_train)\n",
    "    # cv_modelc.best_params_\n",
    "\n",
    "    modelc.fit(X_train, Y_train)\n",
    "\n",
    "    filename = 'C:/Users/dap/model_3.sav'\n",
    "    joblib.dump(modelc, filename)\n",
    "\n",
    "    model_c_score = modelc.score(X_train, Y_train)\n",
    "    model_c_score = model_c_score *100\n",
    "\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now()\n",
    "\n",
    "    cmd = \"update ModelScore set TimeStamp = %s\"\n",
    "\n",
    "    cursor.execute(cmd, timestamp)\n",
    "    cnxn.commit() \n",
    "\n",
    "    modela_score = float(\"{:.2f}\".format(model_a_score))\n",
    "\n",
    "    cmd = \"update ModelScore set ModelA_score = %s\"\n",
    "\n",
    "    cursor.execute(cmd, modela_score)\n",
    "    cnxn.commit() \n",
    "\n",
    "    modelb_score = float(\"{:.2f}\".format(model_b_score))\n",
    "\n",
    "    cmd = \"update ModelScore set ModelB_score = %s\"\n",
    "\n",
    "    cursor.execute(cmd, modelb_score)\n",
    "    cnxn.commit()         \n",
    "\n",
    "    modelc_score = float(\"{:.2f}\".format(model_c_score))\n",
    "\n",
    "    cmd = \"update ModelScore set ModelC_score = %s\"\n",
    "\n",
    "    cursor.execute(cmd, modelc_score)\n",
    "    cnxn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import pymssql\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.externals import joblib\n",
    "from tkinter import *\n",
    "from tkinter import ttk\n",
    "from tkinter import messagebox\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from time import sleep\n",
    "import _mssql\n",
    "\n",
    "\n",
    "# 서버 로그인 정보 json 파일 불러오기\n",
    "login_data=open('C:/Users/dap/login.json').read()\n",
    "login = json.loads(login_data)\n",
    "\n",
    "use_datetime_data=open('C:/Users/dap/use_datetime.json').read()\n",
    "use_datetime = json.loads(use_datetime_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modeling():\n",
    "    \n",
    "    # 서버 연결\n",
    "    cnxn = pymssql.connect(server = login['server'], user = login['user'], password = login['password'],database = login['database'])  \n",
    "    cursor = cnxn.cursor()\n",
    "\n",
    "    # DB WeatherFactor 불러오기\n",
    "    cmd_WF = \"select * from WeatherFactor order by TimeStamp\"\n",
    "    cursor.execute(cmd_WF)\n",
    "\n",
    "    A = pd.read_sql(cmd_WF, cnxn)\n",
    "\n",
    "    # DB YardFactor 불러오기\n",
    "    cmd_YF = \"select * from YardFactor order by TimeStamp\"\n",
    "    cursor.execute(cmd_YF)\n",
    "\n",
    "    B = pd.read_sql(cmd_YF, cnxn)\n",
    "\n",
    "    # DB airkorea 및 reference data 불러오기\n",
    "    cmd_REF = \"select * from ReferenceData order by TimeStamp\"\n",
    "    cursor.execute(cmd_REF)\n",
    "\n",
    "    C = pd.read_sql(cmd_REF, cnxn)\n",
    "\n",
    "    WF = A[A['TimeStamp'] > use_datetime['start']]\n",
    "    YF = B[B['TimeStamp'] > use_datetime['start']]\n",
    "    REF = C[C['TimeStamp'] > use_datetime['start']]\n",
    "\n",
    "    REF = REF[['TimeStamp', 'PM10Yard', 'PM10AirKorea']].reset_index(drop=True)\n",
    "    WF = WF.reset_index(drop=True)\n",
    "    YF = YF.reset_index(drop=True)\n",
    "\n",
    "    # microseconds 삭제\n",
    "    WF['TimeStamp'] = WF['TimeStamp'].apply(lambda x: x.replace(microsecond=0))\n",
    "    YF['TimeStamp'] = YF['TimeStamp'].apply(lambda x: x.replace(microsecond=0))\n",
    "    REF['TimeStamp'] = REF['TimeStamp'].apply(lambda x: x.replace(microsecond=0))\n",
    "\n",
    "    data = pd.merge(REF,WF)\n",
    "    data = pd.merge(data,YF)\n",
    "\n",
    "    data = data[~(data['Temperature'] > 40)]\n",
    "    data = data[~(data['Temperature'] < 0)]\n",
    "    data = data[~(data['WindSpeed'] > 10)]\n",
    "    data = data[~(data['VisibilityDistance'] < 100)]\n",
    "    data = data[~(data['PM10AirKorea'] ==0)]\n",
    "    data = data[~(data['PM10Yard'] == 0)]\n",
    "\n",
    "    datetime = pd.DatetimeIndex(data['TimeStamp'])\n",
    "    data['month'] = datetime.month\n",
    "    data['hour'] = datetime.hour\n",
    "    data['dayofweek'] = datetime.dayofweek\n",
    "\n",
    "    feature_names= ['PM10AirKorea', 'Temperature',\n",
    "               'RelativeHumidity', 'WindSpeed', 'WindDirection', 'SolarIrradiation',\n",
    "               'AtmospherePressure', 'DustCurrent', 'VisibilityDistance',\n",
    "               'YardVibration', 'SurfaceTemperature', 'month', 'hour', 'dayofweek']\n",
    "\n",
    "    X = data[feature_names]\n",
    "    Y = data['PM10Yard']\n",
    "\n",
    "    X_train = X\n",
    "    Y_train = Y\n",
    "\n",
    "   # Create a based model a\n",
    "    modela = RandomForestRegressor(random_state=105,\n",
    "                                  n_estimators=200,\n",
    "                                  max_features=5,\n",
    "                                  max_depth=30)\n",
    "\n",
    "    # param_grid = { \n",
    "    #     'n_estimators': [100,200],\n",
    "    #     'max_features': ['auto', 'sqrt'],\n",
    "    #      'max_features': [5,7,10,14],\n",
    "    #     'max_depth' : [10,20,30,40,50]\n",
    "    # }\n",
    "    # from sklearn.model_selection import GridSearchCV\n",
    "    # cv_modela = GridSearchCV(estimator=modela, param_grid=param_grid, cv= 5)\n",
    "    # cv_modela.fit(X_train, Y_train)\n",
    "    # cv_modela.best_params_\n",
    "\n",
    "    modela.fit(X_train, Y_train)\n",
    "\n",
    "    filename = 'C:/Users/dap/model_1.sav'\n",
    "    joblib.dump(modela, filename)\n",
    "\n",
    "    model_a_score = modela.score(X_train, Y_train)\n",
    "    model_a_score = model_a_score *100\n",
    "\n",
    "    modelb = RandomForestRegressor(random_state=105,\n",
    "                                  n_estimators=200,\n",
    "                                  max_features=10,\n",
    "                                  max_depth=30)\n",
    "\n",
    "    modelb.fit(X_train, Y_train)\n",
    "\n",
    "    filename = 'C:/Users/dap/model_2.sav'\n",
    "    joblib.dump(modelb, filename)\n",
    "\n",
    "    model_b_score = modelb.score(X_train, Y_train)\n",
    "    model_b_score = model_b_score *100\n",
    "\n",
    "    modelc = DecisionTreeRegressor(random_state=105,\n",
    "                                  min_samples_leaf=1,\n",
    "                                  max_depth=5)\n",
    "\n",
    "    # param_grid = { 'min_samples_leaf':[1,2,3],\n",
    "    #            'min_samples_split':[0.01,0.05,0.5,2],\n",
    "    #     'max_depth' : [10,20,30,40,50]\n",
    "    # }\n",
    "    # from sklearn.model_selection import GridSearchCV\n",
    "    # cv_modelc = GridSearchCV(estimator=modelc, param_grid=param_grid, cv= 5)\n",
    "    # cv_modelc.fit(X_train, Y_train)\n",
    "    # cv_modelc.best_params_\n",
    "\n",
    "    modelc.fit(X_train, Y_train)\n",
    "\n",
    "    filename = 'C:/Users/dap/model_3.sav'\n",
    "    joblib.dump(modelc, filename)\n",
    "\n",
    "    model_c_score = modelc.score(X_train, Y_train)\n",
    "    model_c_score = model_c_score *100\n",
    "\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now()\n",
    "\n",
    "    cmd = \"update ModelScore set TimeStamp = %s\"\n",
    "\n",
    "    cursor.execute(cmd, timestamp)\n",
    "    cnxn.commit() \n",
    "\n",
    "    modela_score = float(\"{:.2f}\".format(model_a_score))\n",
    "\n",
    "    cmd = \"update ModelScore set ModelA_score = %s\"\n",
    "\n",
    "    cursor.execute(cmd, modela_score)\n",
    "    cnxn.commit() \n",
    "\n",
    "    modelb_score = float(\"{:.2f}\".format(model_b_score))\n",
    "\n",
    "    cmd = \"update ModelScore set ModelB_score = %s\"\n",
    "\n",
    "    cursor.execute(cmd, modelb_score)\n",
    "    cnxn.commit()         \n",
    "\n",
    "    modelc_score = float(\"{:.2f}\".format(model_c_score))\n",
    "\n",
    "    cmd = \"update ModelScore set ModelC_score = %s\"\n",
    "\n",
    "    cursor.execute(cmd, modelc_score)\n",
    "    cnxn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "from tkinter import ttk\n",
    "import time\n",
    "import threading\n",
    "from tkinter import messagebox\n",
    "\n",
    "try: import tkinter\n",
    "except ImportError:\n",
    "    import Tkinter as tkinter\n",
    "    import ttk\n",
    "\n",
    "else: \n",
    "    from tkinter import ttk\n",
    "\n",
    "app = Tk()\n",
    "app.title(\"AI Dust Prediction Modeling Program\")\n",
    "app.minsize(1035,376)\n",
    "app.maxsize(1035,376)\n",
    "frame = Canvas(app, height=376, width=1035)\n",
    "image1 = PhotoImage(file='C:/Users/dap/gui.png')\n",
    "frame.create_image(1035,0 , anchor = NE, image=image1)\n",
    "frame.pack(side=TOP)\n",
    "\n",
    "def finish_prog():\n",
    "    if messagebox.askokcancel(\"Quit\", \"사용을 종료하시겠습니까?\"):\n",
    "        app.destroy()\n",
    "    \n",
    "def start_thread():\n",
    "    button1['state'] = 'disable'\n",
    "    progress_bar.start()\n",
    "    secondary_thread.start()\n",
    "    app.after(100, check_thread)\n",
    "\n",
    "def check_thread():\n",
    "    if secondary_thread.is_alive():\n",
    "        app.after(100, check_thread)\n",
    "    else:\n",
    "        progress_bar.stop()\n",
    "        button1['state'] ='normal'\n",
    "        messagebox.showinfo('Completed', '예측 모델이 생성되었습니다.')\n",
    "            \n",
    "def arbitrary():\n",
    "    Modeling()   \n",
    "\n",
    "secondary_thread = threading.Thread(target=arbitrary)\n",
    "    \n",
    "button1=Button(frame, text = 'START',fg='White', bg=\"#212933\" ,font=\"Bahnschrift 12\", width=7, command=start_thread)\n",
    "button1.place(x=870, y=235)\n",
    "button2=Button(frame, text = 'EXIT', fg='White', bg=\"#212933\" ,font=\"Bahnschrift 12\",width=7,command=finish_prog)\n",
    "button2.place(x=950, y=235)\n",
    "progress_bar = ttk.Progressbar(frame, orient='horizontal', length=550, mode ='determinate')\n",
    "progress_bar.place(x=290, y=240)\n",
    "\n",
    "app.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
